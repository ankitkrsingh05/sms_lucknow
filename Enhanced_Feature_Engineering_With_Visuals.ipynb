{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "89756243", "cell_type": "markdown", "source": "\n# Feature Engineering & Data Preprocessing (With Visuals + Model Impact)\n\nThis notebook extends the preprocessing notebook by:\n1. Adding **visualizations** for key preprocessing steps  \n2. Showing **model performance before vs after preprocessing**  \n3. Keeping the notebook **trainer & interview ready**  \n\nA **separate theory document** accompanies this notebook.\n", "metadata": {}}, {"id": "83f6e593", "cell_type": "markdown", "source": "\n## Dataset: California Housing Dataset\nTarget: `MedHouseValue`\n", "metadata": {}}, {"id": "45516da7", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = fetch_california_housing(as_frame=True)\ndf = data.frame.copy()\ndf.head()\n", "outputs": []}, {"id": "c3c32ec2", "cell_type": "markdown", "source": "\n## 1. Feature Distribution (Before Preprocessing)\n\nUnderstanding skewness and scale helps decide:\n- Log / Power transform\n- Scaling strategy\n", "metadata": {}}, {"id": "0fcff404", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nplt.figure(figsize=(6,4))\nsns.histplot(df[\"Population\"], kde=True)\nplt.title(\"Population Distribution (Raw)\")\nplt.show()\n", "outputs": []}, {"id": "8f5cd1a7", "cell_type": "markdown", "source": "\n## 2. Missing Value Imputation \u2013 Visual Impact\n\nWe introduce missing values artificially to visualize imputation effects.\n", "metadata": {}}, {"id": "ce1686f6", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nfrom sklearn.impute import SimpleImputer\n\ndf_missing = df.copy()\ndf_missing.iloc[::15, df.columns.get_loc(\"Population\")] = np.nan\n\nimputer = SimpleImputer(strategy=\"median\")\ndf_imputed = pd.DataFrame(imputer.fit_transform(df_missing), columns=df.columns)\n\nplt.figure(figsize=(6,4))\nsns.kdeplot(df_missing[\"Population\"], label=\"With NaNs\")\nsns.kdeplot(df_imputed[\"Population\"], label=\"After Imputation\")\nplt.legend()\nplt.show()\n", "outputs": []}, {"id": "b0367702", "cell_type": "markdown", "source": "\n## 3. Scaling Effect Visualization\n\nStandardization vs Normalization\n", "metadata": {}}, {"id": "8a06ac21", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nstd = StandardScaler()\nmm = MinMaxScaler()\n\npop_std = std.fit_transform(df_imputed[[\"Population\"]])\npop_mm = mm.fit_transform(df_imputed[[\"Population\"]])\n\nplt.figure(figsize=(6,4))\nsns.kdeplot(pop_std.flatten(), label=\"Standardized\")\nsns.kdeplot(pop_mm.flatten(), label=\"Normalized\")\nplt.legend()\nplt.show()\n", "outputs": []}, {"id": "1fac6406", "cell_type": "markdown", "source": "\n## 4. Power Transformation Visualization\n", "metadata": {}}, {"id": "7e576e94", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nfrom sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method=\"yeo-johnson\")\npop_power = pt.fit_transform(df_imputed[[\"Population\"]])\n\nplt.figure(figsize=(6,4))\nsns.kdeplot(df_imputed[\"Population\"], label=\"Original\")\nsns.kdeplot(pop_power.flatten(), label=\"Power Transformed\")\nplt.legend()\nplt.show()\n", "outputs": []}, {"id": "884717a4", "cell_type": "markdown", "source": "\n## 5. Model Performance: Before vs After Preprocessing\n\nWe compare:\n- Raw data\n- Preprocessed data\n\nMetric: **R\u00b2 Score**\n", "metadata": {}}, {"id": "749d5f6f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nX = df.drop(columns=[\"MedHouseValue\"])\ny = df[\"MedHouseValue\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nlr_raw = LinearRegression()\nlr_raw.fit(X_train, y_train)\nr2_raw = r2_score(y_test, lr_raw.predict(X_test))\n\n# Preprocessed\nX_scaled = StandardScaler().fit_transform(df_imputed.drop(columns=[\"MedHouseValue\"]))\nX_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\nlr_scaled = LinearRegression()\nlr_scaled.fit(X_train_s, y_train_s)\nr2_scaled = r2_score(y_test_s, lr_scaled.predict(X_test_s))\n\nr2_raw, r2_scaled\n", "outputs": []}, {"id": "9389af22", "cell_type": "markdown", "source": "\n### Interpretation\n- Preprocessing improves numerical stability\n- Gradient-based models benefit the most\n", "metadata": {}}, {"id": "06063912", "cell_type": "markdown", "source": "\n## Final Takeaways\n\n\u2714 Visual intuition for preprocessing  \n\u2714 Clear impact on model performance  \n\u2714 End-to-end real-world workflow  \n\nUse this notebook for:\n- Teaching\n- Interview preparation\n- ML foundations revision\n", "metadata": {}}]}