{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6eb7f0",
   "metadata": {},
   "source": [
    "\n",
    "# Complete Feature Engineering & Data Preprocessing (End-to-End)\n",
    "\n",
    "This notebook is a **trainer-level, exhaustive reference** for **data preprocessing and feature engineering**\n",
    "using a **real-world dataset**.\n",
    "\n",
    "You will learn:\n",
    "- What each technique does\n",
    "- When to use it\n",
    "- Why it matters for model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de659247",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset: California Housing (Real-Life)\n",
    "\n",
    "Target: `MedHouseValue`\n",
    "\n",
    "Why this dataset?\n",
    "- Numerical-heavy (perfect for preprocessing)\n",
    "- Realistic distributions\n",
    "- Industry-standard dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1234b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame.copy()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb78bfe",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Basic EDA and Descriptive statistics\n",
    "\n",
    "Before preprocessing, always inspect:\n",
    "- Distribution\n",
    "- Scale differences\n",
    "- Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02845ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320bd2c",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Missing Value Imputation\n",
    "\n",
    "### Why?\n",
    "Most ML algorithms **cannot handle NaN values**.\n",
    "\n",
    "### Techniques Covered\n",
    "- Mean\n",
    "- Median\n",
    "- Mode\n",
    "- KNN Imputation\n",
    "- Iterative Imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97afdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Introduce artificial missing values\n",
    "df_missing = df.copy()\n",
    "df_missing.iloc[::10, 0] = np.nan\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy=\"mean\")\n",
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "mode_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "iter_imputer = IterativeImputer(random_state=42)\n",
    "\n",
    "df_mean = pd.DataFrame(mean_imputer.fit_transform(df_missing), columns=df.columns)\n",
    "df_median = pd.DataFrame(median_imputer.fit_transform(df_missing), columns=df.columns)\n",
    "df_mode = pd.DataFrame(mode_imputer.fit_transform(df_missing), columns=df.columns)\n",
    "df_knn = pd.DataFrame(knn_imputer.fit_transform(df_missing), columns=df.columns)\n",
    "df_iter = pd.DataFrame(iter_imputer.fit_transform(df_missing), columns=df.columns)\n",
    "\n",
    "df_mean.isnull().sum().head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc919ef",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Feature Scaling\n",
    "\n",
    "### Standardization vs Normalization\n",
    "\n",
    "**Standardization (Z-score):**\n",
    "- Mean = 0, Std = 1\n",
    "- Used for GD-based models\n",
    "\n",
    "**Normalization (MinMax):**\n",
    "- Range [0,1]\n",
    "- Used for distance-based models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "df_std = pd.DataFrame(std_scaler.fit_transform(df_mean), columns=df.columns)\n",
    "df_norm = pd.DataFrame(minmax_scaler.fit_transform(df_mean), columns=df.columns)\n",
    "\n",
    "df_std.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64036ec",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Power Transformation\n",
    "\n",
    "### Why?\n",
    "- Handles skewed distributions\n",
    "- Makes data more Gaussian\n",
    "\n",
    "Techniques:\n",
    "- Box-Cox (positive only)\n",
    "- Yeo-Johnson (allows zero/negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfc4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method=\"yeo-johnson\")\n",
    "df_power = pd.DataFrame(pt.fit_transform(df_mean), columns=df.columns)\n",
    "df_power.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce4d23",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Encoding Categorical Data\n",
    "\n",
    "We simulate categorical data for demonstration.\n",
    "\n",
    "Techniques:\n",
    "- Ordinal Encoding\n",
    "- One-Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "df_cat = df_mean.copy()\n",
    "df_cat[\"Income_Category\"] = pd.cut(\n",
    "    df_cat[\"MedInc\"],\n",
    "    bins=[0,2,4,6,8,15],\n",
    "    labels=[\"Low\",\"Medium\",\"High\",\"Very High\",\"Ultra\"]\n",
    ")\n",
    "\n",
    "ordinal_enc = OrdinalEncoder(categories=[[\"Low\",\"Medium\",\"High\",\"Very High\",\"Ultra\"]])\n",
    "df_cat[\"Income_Ordinal\"] = ordinal_enc.fit_transform(df_cat[[\"Income_Category\"]])\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ohe_features = ohe.fit_transform(df_cat[[\"Income_Category\"]])\n",
    "ohe_df = pd.DataFrame(ohe_features, columns=ohe.get_feature_names_out())\n",
    "\n",
    "df_cat[[\"Income_Category\",\"Income_Ordinal\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be18120",
   "metadata": {},
   "source": [
    "\n",
    "## 6. ColumnTransformer & FunctionTransformer\n",
    "\n",
    "### Why?\n",
    "- Apply different transformations to different columns\n",
    "- Apply custom logic inside sklearn workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def log_transform(x):\n",
    "    return np.log1p(x)\n",
    "\n",
    "log_transformer = FunctionTransformer(log_transform)\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    (\"log_population\", log_transformer, [\"Population\"]),\n",
    "    (\"scale_income\", StandardScaler(), [\"MedInc\"])\n",
    "])\n",
    "\n",
    "ct.fit_transform(df_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142b056",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Binning & Binarization\n",
    "\n",
    "### Binning\n",
    "- Converts continuous → discrete\n",
    "- Useful for tree models & interpretability\n",
    "\n",
    "### Binarization\n",
    "- Converts feature to 0/1 based on threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ca5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer, Binarizer\n",
    "\n",
    "binning = KBinsDiscretizer(n_bins=4, encode=\"ordinal\", strategy=\"quantile\")\n",
    "df_mean[\"HouseAge_Binned\"] = binning.fit_transform(df_mean[[\"HouseAge\"]])\n",
    "\n",
    "binarizer = Binarizer(threshold=5)\n",
    "df_mean[\"Income_Binary\"] = binarizer.fit_transform(df_mean[[\"MedInc\"]])\n",
    "\n",
    "df_mean[[\"HouseAge\",\"HouseAge_Binned\",\"Income_Binary\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb18aa",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Outlier Detection & Removal\n",
    "\n",
    "### Z-score\n",
    "- Assumes Gaussian distribution\n",
    "\n",
    "### IQR\n",
    "- Robust to skewed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "z_scores = np.abs(zscore(df_mean))\n",
    "df_z = df_mean[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "Q1 = df_mean.quantile(0.25)\n",
    "Q3 = df_mean.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df_iqr = df_mean[~((df_mean < (Q1 - 1.5 * IQR)) | (df_mean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "(df_mean.shape, df_z.shape, df_iqr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21588a73",
   "metadata": {},
   "source": [
    "\n",
    "## Final Summary\n",
    "\n",
    "### Preprocessing Techniques Covered\n",
    "✔ Mean / Median / Mode Imputation  \n",
    "✔ KNN & Iterative Imputation  \n",
    "✔ Standardization & Normalization  \n",
    "✔ Power Transformation  \n",
    "✔ Ordinal & One-Hot Encoding  \n",
    "✔ ColumnTransformer & FunctionTransformer  \n",
    "✔ Binning & Binarization  \n",
    "✔ Outlier removal (Z-score & IQR)\n",
    "\n",
    "This notebook reflects **real-world ML preprocessing workflows** used in industry.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
